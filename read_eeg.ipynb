{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lib import BrennanDataset\n",
    "\n",
    "base_dir = Path(\"/ocean/projects/cis240129p/shared/data/eeg_alice\")\n",
    "subjects_used = [\"S04\", \"S13\", \"S19\"]  # exclude 'S05' - less channels\n",
    "\n",
    "# ds = BrennanDataset(\n",
    "#     root_dir=base_dir,\n",
    "#     phoneme_dir=base_dir / \"phonemes\",\n",
    "#     idx=\"S01\",\n",
    "#     phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S13.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368274  =      0.000 ...   736.548 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S19.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 373374  =      0.000 ...   746.748 secs...\n",
      "Train dataset length: 5109, Test dataset length: 1278\n"
     ]
    }
   ],
   "source": [
    "def create_datasets(subjects, base_dir):\n",
    "    train_datasets = []\n",
    "    test_datasets = []\n",
    "    for subject in subjects:\n",
    "        dataset = BrennanDataset(\n",
    "            root_dir=base_dir,\n",
    "            phoneme_dir=base_dir / \"phonemes\",\n",
    "            idx=subject,\n",
    "            phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "        )\n",
    "        num_data_points = len(dataset)\n",
    "\n",
    "        # Split indices into train and test sets\n",
    "        split_index = int(num_data_points * 0.8)\n",
    "        train_indices = list(range(split_index))\n",
    "        test_indices = list(range(split_index, num_data_points))\n",
    "\n",
    "        # Create Subset datasets using indices\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "        train_datasets.append(train_dataset)\n",
    "        test_datasets.append(test_dataset)\n",
    "    return train_datasets, test_datasets\n",
    "\n",
    "\n",
    "train_ds, test_ds = create_datasets(subjects_used, base_dir)\n",
    "train_dataset = ConcatDataset(train_ds)\n",
    "test_dataset = ConcatDataset(test_ds)\n",
    "print(\n",
    "    f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function that handles different types of data in a batch.\n",
    "    It dynamically creates batches by converting arrays or lists to tensors and\n",
    "    applies padding to variable-length sequences.\n",
    "    \"\"\"\n",
    "    batch_dict = {}\n",
    "    for key in batch[0].keys():\n",
    "        batch_items = [item[key] for item in batch]\n",
    "        if isinstance(batch_items[0], np.ndarray) or isinstance(\n",
    "            batch_items[0], torch.Tensor\n",
    "        ):\n",
    "            if isinstance(batch_items[0], np.ndarray):\n",
    "                batch_items = [torch.tensor(b) for b in batch_items]\n",
    "            if len(batch_items[0].shape) > 0:\n",
    "                batch_dict[key] = torch.nn.utils.rnn.pad_sequence(\n",
    "                    batch_items, batch_first=True  # pad with zeros\n",
    "                )\n",
    "            else:\n",
    "                batch_dict[key] = torch.stack(batch_items)\n",
    "        else:\n",
    "            batch_dict[key] = batch_items\n",
    "\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "train_dataloder = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloder = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label <class 'str'>\n",
      "audio_feats (104, 128) <class 'numpy.ndarray'>\n",
      "audio_raw (16735,) <class 'numpy.ndarray'>\n",
      "eeg_raw (520, 62) <class 'numpy.ndarray'>\n",
      "eeg_feats (159, 310) <class 'numpy.ndarray'>\n",
      "phonemes (104,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k, v in item.items():\n",
    "    try:\n",
    "        print(k, v.shape, type(v))\n",
    "    except:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "1\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "2\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "3\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "4\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "i = 0\n",
    "for batch in train_dataloder:\n",
    "    print(i)\n",
    "    for k, v in batch.items():\n",
    "        try:\n",
    "            print(k, v.shape, type(v))\n",
    "        except:\n",
    "            print(k, type(v))\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
